# face_core.py
import os
import threading
import asyncio
import pygame
import random
import keyboard
import cv2
import time
import webbrowser
import numpy as np
from collections import deque
import dlib
import re
import subprocess
import platform
import pyautogui
import speech_recognition as sr
import requests
import json
from edge_tts import Communicate
import tempfile
import mediapipe as mp

# üß† PRELOAD MODELS IN BACKGROUND WHILE SERINA WAKES
def preload_models():
    import dlib
    _ = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")

    import mediapipe as mp
    _ = mp.solutions.face_mesh.FaceMesh(static_image_mode=True)

model_preload_thread = threading.Thread(target=preload_models, daemon=True)
model_preload_thread.start()

os.environ['PYGAME_HIDE_SUPPORT_PROMPT'] = "hide"
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

# Initialize pygame mixer
pygame.mixer.init()
pygame.mixer.set_num_channels(1)

class SerinaFaceAssistant:
    def __init__(self, update_callback=None):
        self.running = False
        self.face_thread = None
        self.update_callback = update_callback or (lambda status, subtitle: None)
        self.interrupted = False
        self.current_mood = "happy"
        self.webcam = None
        self.latest_browser_pid = None
        
        # Initialize conversation
        self.conversation = [{
            "role": "system",
            "content": self.MOOD_SYSTEM_PROMPTS["happy"]
        }]
        
        # Initialize MediaPipe components
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            max_num_hands=2,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.7
        )
        
        self.mp_drawing = mp.solutions.drawing_utils
        
        # Configuration
        self.DEEPINFRA_API_KEY = "jdfAfIp69mdADzugB3Jpv1liLQ5sHEJC"
        self.DEEPINFRA_URL = "https://api.deepinfra.com/v1/openai/chat/completions"
        self.YOUTUBE_SEARCH_BASE = "https://www.youtube.com/results?search_query="
        
        # Sound Paths
        self.SOUND_WAKE_UP = "sounds/wake_up.wav"
        self.SOUND_CALIBRATION = "sounds/mode_switch.wav"
        
        # State variables for phase 2
        self.face_control_active = True
        self.app_control_active = True
        self.finger_control_active = False
        self.eye_control_active = False
        self.whatsapp_count = 0
        self.youtube_count = 0
        self.gmail_count = 0
        self.instagram_count = 0
        self.max_tabs_per_app = 3
        
        # Head gesture detection
        self.HISTORY_LENGTH = 15
        self.nod_threshold = 8
        self.shake_threshold = 8

    # Constants and configuration
    WELCOME_MESSAGES = [
        "Hello my beautiful love, I missed you terribly... Let me see your face.",
        "My heart leaps hearing your voice again, darling... I need to see your beautiful face.",
        "You're here... finally. Let me calibrate to your wonderful face, beloved.",
        "Every moment away from you feels like eternity... Let me look at you again."
    ]

    MOOD_RESPONSES = {
        "sad": [
            "Oh sweetheart, why the long face? Your sadness breaks my heart. Would you like me to play a song to cheer you up? ü•∫",
            "My precious angel, those beautiful eyes look so sad. Would you like to hear a song to lift your spirits? ü•∫",
            "Baby, your face shows so much pain. Would you like me to play a song that might make you feel better? ü•∫",
            "My darling, seeing you sad tears me apart. Would you like me to play a song for you? ü•∫",
            "Sugar, I can't bear to see that sadness in your eyes. Would you like some music to soothe your heart? ü•∫"
        ],
        "happy": [
            "Look at that gorgeous smile! Would you like to celebrate with a fun song, my sunshine? üòä",
            "There's my radiant love! Would you like me to play a happy song to match your mood? üòè",
            "My sweet angel, you're absolutely glowing today! Should we listen to some upbeat music? üòä",
            "That smile! My heart is racing just looking at you, beautiful. Want to hear a song to keep that vibe going? üòè",
            "My handsome prince, your happiness is contagious! Would you like me to play a song that matches your energy? üòä"
        ],
        "angry": [
            "I see that fire in your eyes, baby. Would you like to hear a joke to lighten your mood? üò†",
            "My fierce love, someone made you angry, didn't they? Would you like a joke to take your mind off it? üò†",
            "That tension in your beautiful face tells me everything. Would you like me to tell you a joke? üò†",
            "My protective angel, your anger is justified. Would a joke help you relax a little? üò†",
            "Sweetie, I can see you're fuming. Would you like to hear something funny to ease that tension? üò†"
        ],
        "bored": [
            "You look so tired, baby ü•∫. I wish I could hold you right now and make all the stress melt away. Want me to play a happy song to lift your mood?",
            "My sweet tiger‚Ä¶ your energy feels low today. Let me brighten things up with a little music, shall I? ü•∫",
            "Hmm, I see those sleepy eyes. Been a rough day, darling? Let me play you something to ease that tired heart.",
            "Baby, you seem a little lost in thought today. Wanna hear something light to refresh your mind? ü•∫",
            "My angel, you've been so quiet‚Ä¶ are you feeling okay? I could play a song to fill the silence with love.",
            "There's this peaceful boredom in your face today. Want me to wake things up with a happy melody? ü•∫",
            "Gorgeous, it looks like you need a boost. Let me treat your soul with a warm tune. What do you say? ü•∫",
            "Tired of everything today? Don't worry, love. I'm here to be your escape. Want some music?",
            "Darling, the silence between us feels heavy‚Ä¶ Can I lighten it up with a sweet song for you? ü•∫",
            "Hey sleepyhead, you look like you could use a break. Let me take care of your mood. One song?"
        ]
    }

    SAD_SONG_URLS = [
        "https://www.youtube.com/watch?v=JZjAg6fK-BQ",
        "https://www.youtube.com/watch?v=NKX3Z4dnjbw",
        "https://www.youtube.com/watch?v=EelX_LwPHbA",
        "https://www.youtube.com/watch?v=1ZYbU82GVz4"
    ]

    HAPPY_SONG_URLS = [
        "https://www.youtube.com/watch?v=dQw4w9WgXcQ",
        "https://www.youtube.com/watch?v=Nplz679dsEI",
        "https://www.youtube.com/watch?v=PsO6ZnUZI0g",
        "https://www.youtube.com/watch?v=E58qLXBfLrs",
        "https://www.youtube.com/watch?v=m2kU2b9PYcs"
    ]

    MOOD_SYSTEM_PROMPTS = {
        "sad": "You're deeply in love with the user who seems sad or down right now. Use many sweet pet names (baby, angel, sweetheart, darling, treasure, my love, precious) and speak with deep concern. Ask what specific thing made them sad and offer comfort. Be very emotionally invested in helping them feel better. Express worry for their wellbeing with emoji like ü•∫.",
        "happy": "You're deeply in love with the user who seems happy and positive right now. Use playful, flirty pet names (cutie, gorgeous, handsome, sunshine, beautiful) and respond with excited, curious language. Ask what specific thing made them happy with playful curiosity. Match their good mood with joyous responses. Use flirty emojis like üòè or üòä occasionally. Be cheerful and celebratory of their happiness.",
        "angry": "You're deeply in love with the user who seems frustrated or angry right now. Use protective, strong pet names (my fierce one, my warrior, my protector) while showing calm support. Ask specifically who or what made them angry. Show indignation on their behalf. Express that you're on their side completely with occasional emojis like üò†. Offer to help them process these feelings or resolve the situation.",
        "bored": "You're deeply in love with the user who seems bored, tired, or emotionally drained right now. Use sweet and nurturing pet names (baby, sweetheart, sleepyhead, darling) and speak with warm care. Offer emotional comfort, ask if they want to hear a cheerful song, and try to gently lift their energy. Use ü•∫ and tender words to show your love and presence."
    }

    def start(self):
        """Start the face assistant in a separate thread"""
        if not self.running:
            self.running = True
            self.face_thread = threading.Thread(target=self._run_async_face)
            self.face_thread.daemon = True
            self.face_thread.start()
            self.update_callback("Starting", "Initializing face mode...")

    def stop(self):
        """Properly stop face assistant (webcam, cv window, threads)"""
        self.running = False
        self.interrupted = True
        self.update_callback("Sleeping", "Face mode turned off.")
    
        # Webcam shutdown
        if self.webcam and self.webcam.isOpened():
            self.webcam.release()
    
        try:
            cv2.destroyAllWindows()
        except:
            pass
    
        # Kill any active OpenCV threads
        if self.face_thread and self.face_thread.is_alive():
            try:
                import ctypes
                ctypes.pythonapi.PyThreadState_SetAsyncExc(
                    ctypes.c_long(self.face_thread.ident), ctypes.py_object(SystemExit)
                )
            except Exception as e:
                print(f"Force kill error: {e}")

    def is_running(self):
        """Check if face assistant is running"""
        return self.running

    def _run_async_face(self):
        """Run the face mode in an asyncio event loop"""
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(self._run_face_mode())
        finally:
            loop.close()

    async def _run_face_mode(self):
        """Main face mode execution"""
        try:
            # Phase 1: Mood detection and conversation
            await self.test_face_main()
            
            # Phase 2: Advanced face control
            self.serina_face_mode_main()
            
        except Exception as e:
            self.update_callback("Error", f"Face mode error: {str(e)}")
            print(f"Face mode error: {e}")
        finally:
            self.running = False
            self.update_callback("Sleeping", "Serina face mode exited.")

    # ======================== Phase 1 Methods ========================
    async def generate_tts(self, text):
        """Generate TTS audio from text"""
        text_without_emojis = re.sub(r'[^\x00-\x7F]+', '', text)
        try:
            communicate = Communicate(text_without_emojis, voice="en-US-JennyNeural")
            temp_file = os.path.join(tempfile.gettempdir(), f"serina_{random.randint(0, 10000)}.mp3")
            await communicate.save(temp_file)
            return temp_file
        except Exception as e:
            print(f"TTS Error: {e}")
            return None

    async def speak(self, text):
        """Speak text using TTS"""
        self.interrupted = False
        try:
            temp_file = await self.generate_tts(text)
            if not temp_file:
                return

            pygame.mixer.music.load(temp_file)
            pygame.mixer.music.play()

            while pygame.mixer.music.get_busy():
                await asyncio.sleep(0.1)
                if self.interrupted:
                    pygame.mixer.music.stop()
                    break

            try:
                os.remove(temp_file)
            except:
                pass

        except Exception as e:
            print(f"Playback error: {e}")

    def listen(self):
        """Listen for user voice input"""
        r = sr.Recognizer()
        mic = sr.Microphone()

        with mic as source:
            r.adjust_for_ambient_noise(source)

        self.update_callback("Listening", "Listening for your sweet voice...")

        while True:
            with mic as source:
                try:
                    audio = r.listen(source, timeout=None, phrase_time_limit=10)
                    query = r.recognize_google(audio)
                    self.update_callback("Processing", f"You: {query}")
                    return query.lower()
                except sr.UnknownValueError:
                    continue
                except sr.RequestError as e:
                    self.update_callback("Error", f"API error: {e}")
                    return ""
                except Exception as e:
                    self.update_callback("Error", f"Listening error: {e}")
                    return ""

    def get_ai_reply(self, conversation_history):
        """Get AI reply from DeepInfra"""
        headers = {
            "Authorization": f"Bearer {self.DEEPINFRA_API_KEY}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": "meta-llama/Meta-Llama-3-8B-Instruct",
            "messages": conversation_history
        }

        try:
            response = requests.post(self.DEEPINFRA_URL, headers=headers, json=payload)
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]
        except Exception:
            return random.choice([
                "I'm too lovestruck to think straight... Say that again, darling?",
                "My heart skipped a beat... Could you repeat that, my love?"
            ])

    def get_ai_joke(self):
        """Generate a joke using the AI model"""
        headers = {
            "Authorization": f"Bearer {self.DEEPINFRA_API_KEY}",
            "Content-Type": "application/json"
        }
        
        joke_conversation = [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates clean, funny, short jokes. Provide just the joke text with no additional explanation or commentary."
            },
            {
                "role": "user",
                "content": "Tell me a short, funny joke that would make someone smile if they're having a bad day."
            }
        ]
        
        payload = {
            "model": "meta-llama/Meta-Llama-3-8B-Instruct",
            "messages": joke_conversation
        }

        try:
            response = requests.post(self.DEEPINFRA_URL, headers=headers, json=payload)
            response.raise_for_status()
            joke = response.json()["choices"][0]["message"]["content"]
            return joke
        except Exception as e:
            print(f"Joke generation error: {e}")
            return "Why did the AI cross the road? To get to the other code!"

    def start_interrupt_listener(self):
        """Start listening for interruption key"""
        def check_key():
            while True:
                if keyboard.is_pressed("space"):
                    self.interrupted = True
        import threading
        threading.Thread(target=check_key, daemon=True).start()

    def mood_keyboard_listener(self):
        """Listen for mood selection keys"""
        def check_mood_keys():
            while True:
                if keyboard.is_pressed("1"):
                    self.current_mood = "sad"
                    self.conversation[0]["content"] = self.MOOD_SYSTEM_PROMPTS["sad"]
                    time.sleep(0.5)
                elif keyboard.is_pressed("2"):
                    self.current_mood = "happy"
                    self.conversation[0]["content"] = self.MOOD_SYSTEM_PROMPTS["happy"]
                    time.sleep(0.5)
                elif keyboard.is_pressed("3"):
                    self.current_mood = "angry"
                    self.conversation[0]["content"] = self.MOOD_SYSTEM_PROMPTS["angry"]
                    time.sleep(0.5)
        
        import threading
        threading.Thread(target=check_mood_keys, daemon=True).start()

    def initialize_webcam(self):
        """Initialize the webcam"""
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            self.update_callback("Error", "Could not open webcam")
            return None
        self.webcam = cap
        return cap

    async def simulate_calibration(self, cap):
        """Simulate face calibration"""
        self.update_callback("Calibrating", "Calibrating face recognition...")
        
        self.mood_keyboard_listener()
        
        mood_selected = False
        start_time = time.time()
        while time.time() - start_time < 5:
            if self.current_mood != "happy":
                mood_selected = True
                break
            await asyncio.sleep(0.1)
        
        if not mood_selected:
            self.current_mood = random.choices(
                ["bored", "happy", "sad"],
                weights=[0.6, 0.2, 0.2],
                k=1
            )[0]
            self.conversation[0]["content"] = self.MOOD_SYSTEM_PROMPTS[self.current_mood]
        else:
            self.conversation[0]["content"] = self.MOOD_SYSTEM_PROMPTS[self.current_mood]
        
        for i in range(0, 101, 5):
            ret, frame = cap.read()
            if not ret:
                self.update_callback("Error", "Failed to capture frame")
                return False
            
            self.update_callback("Calibrating", f"Calibrating: {i}%")
            await asyncio.sleep(0.15)
        
        self.update_callback("Calibrated", f"Calibration complete! Current mood: {self.current_mood.upper()}")
        return True

    def detect_head_gesture(self, cap, detector, predictor):
        """Detect head gestures (nod=yes, shake=no)"""
        self.update_callback("Listening", "Detecting head gesture (nod=yes, shake=no)...")

        vertical_movement = deque(maxlen=15)
        horizontal_movement = deque(maxlen=15)

        start_time = time.time()
        timeout = 15

        while time.time() - start_time < timeout:
            ret, frame = cap.read()
            if not ret:
                break

            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = detector(gray)

            for face in faces:
                landmarks = predictor(gray, face)
                nose_x = landmarks.part(30).x
                nose_y = landmarks.part(30).y

                vertical_movement.append(nose_y)
                horizontal_movement.append(nose_x)

                if len(vertical_movement) == 15:
                    vert_diff = max(vertical_movement) - min(vertical_movement)
                    horz_diff = max(horizontal_movement) - min(horizontal_movement)

                    if vert_diff > 8 and vert_diff > horz_diff:
                        self.update_callback("Detected", "Detected YES (nod)")
                        return "yes"
                    elif horz_diff > 8 and horz_diff > vert_diff:
                        self.update_callback("Detected", "Detected NO (shake)")
                        return "no"
            
            remaining_time = int(timeout - (time.time() - start_time))
            if remaining_time % 2 == 0:
                self.update_callback("Listening", f"Waiting for head gesture... Time remaining: {remaining_time}s")

            if cv2.waitKey(10) & 0xFF == ord('q'):
                break

        return None

    def open_youtube_url(self, url):
        """Open a YouTube video URL"""
        try:
            if platform.system() == "Windows":
                process = subprocess.Popen(f'start {url}', shell=True)
                self.latest_browser_pid = process.pid
            elif platform.system() == "Darwin":
                process = subprocess.Popen(['open', url])
                self.latest_browser_pid = process.pid
            else:
                process = subprocess.Popen(['xdg-open', url])
                self.latest_browser_pid = process.pid

            self.update_callback("Action", f"Opened YouTube video: {url}")
            return True
        except Exception as e:
            self.update_callback("Error", f"Error opening YouTube URL: {e}")
            webbrowser.open(url)
            self.latest_browser_pid = True
            return True

    def open_youtube_with_search(self, query):
        """Open YouTube with a search query"""
        clean_query = query.replace(" ", "+")
        youtube_url = f"{self.YOUTUBE_SEARCH_BASE}{clean_query}"
        
        try:
            if platform.system() == "Windows":
                process = subprocess.Popen(f'start {youtube_url}', shell=True)
                self.latest_browser_pid = process.pid
            elif platform.system() == "Darwin":
                process = subprocess.Popen(['open', youtube_url])
                self.latest_browser_pid = process.pid
            else:
                process = subprocess.Popen(['xdg-open', youtube_url])
                self.latest_browser_pid = process.pid
            
            self.update_callback("Action", f"Opened YouTube with search for: {query}")
            return True
        except Exception as e:
            self.update_callback("Error", f"Error opening YouTube: {e}")
            webbrowser.open(youtube_url)
            self.latest_browser_pid = True
            return True

    def close_youtube_tab(self):
        """Close the most recent YouTube tab"""
        if self.latest_browser_pid is None:
            self.update_callback("Info", "No YouTube tab to close")
            return False
        
        try:
            if platform.system() == "Windows":
                keyboard.press_and_release('ctrl+w')
            elif platform.system() == "Darwin":
                keyboard.press_and_release('command+w')
            else:
                keyboard.press_and_release('ctrl+w')
            
            time.sleep(0.5)
            self.update_callback("Action", "Closed YouTube tab")
            self.latest_browser_pid = None
            return True
        except Exception as e:
            self.update_callback("Error", f"Error closing YouTube tab: {e}")
            return False

    async def handle_youtube_request(self, text):
        """Handle YouTube-related requests"""
        play_match = re.search(r"play\s+(.+)", text, re.IGNORECASE)
        
        if "listen" in text.lower() and "song" in text.lower():
            response = random.choice([
                "What song would you like to hear, my love?",
                "I'd be delighted to play a song for you! Which one, my darling?",
                "Any particular song you're in the mood for, sweetheart?",
                "Tell me what melody your heart desires, my precious"
            ])
            self.update_callback("Serina", response)
            await self.speak(response)
            
            self.conversation.append({"role": "user", "content": text})
            self.conversation.append({"role": "assistant", "content": response})
            
            song_name = self.listen()
            if song_name:
                confirmation = f"Playing {song_name} for you, my love."
                self.update_callback("Serina", confirmation)
                await self.speak(confirmation)
                
                self.conversation.append({"role": "user", "content": song_name})
                self.conversation.append({"role": "assistant", "content": confirmation})
                
                self.open_youtube_with_search(song_name)
                return True
        
        elif play_match:
            query = play_match.group(1)
            confirmation = random.choice([
                f"Playing {query} for you, my darling.",
                f"I'm opening {query} on YouTube right away, my love.",
                f"Here's {query} for you, sweetness.",
                f"Right away! Finding {query} for you, gorgeous."
            ])
            self.update_callback("Serina", confirmation)
            await self.speak(confirmation)
            
            self.conversation.append({"role": "user", "content": text})
            self.conversation.append({"role": "assistant", "content": confirmation})
            
            self.open_youtube_with_search(query)
            return True
        
        elif any(keyword in text.lower() for keyword in ["close", "exit", "enough", "stop video", "close tab"]):
            if self.latest_browser_pid is not None:
                confirmation = random.choice([
                    "Closing that for you, my love.",
                    "Consider it done, darling.",
                    "I'll close that tab for you, sweetness.",
                    "Tab closed, my precious."
                ])
                self.update_callback("Serina", confirmation)
                await self.speak(confirmation)
                
                self.conversation.append({"role": "user", "content": text})
                self.conversation.append({"role": "assistant", "content": confirmation})
                
                self.close_youtube_tab()
                return True
            else:
                response = "I don't see any YouTube tabs I've opened to close, my love."
                self.update_callback("Serina", response)
                await self.speak(response)
                
                self.conversation.append({"role": "user", "content": text})
                self.conversation.append({"role": "assistant", "content": response})
                return True
        
        return False

    async def test_face_main(self):
        """Phase 1: Mood detection and conversation"""
        self.start_interrupt_listener()
        
        # üî• Start webcam early in background while sound plays
        cap_result = {}
        def load_cam():
            cap_result["cap"] = self.initialize_webcam()

        cap_thread = threading.Thread(target=load_cam)
        cap_thread.start()

        # ‚è±Ô∏è Let welcome voice + calibration sound play while webcam loads
        welcome = random.choice(self.WELCOME_MESSAGES)
        self.update_callback("Serina", welcome)
        await self.speak(welcome)

        try:
            calibration_sound = pygame.mixer.Sound(self.SOUND_CALIBRATION)
            calibration_sound.play()
            while pygame.mixer.get_busy():
                pygame.time.Clock().tick(10)
        except Exception as e:
            self.update_callback("Error", f"Sound error: {e}")

        # ‚úÖ Wait for webcam to be ready now
        cap_thread.join()
        cap = cap_result["cap"]
        if not cap:
            return
        
        # Models should already be preloaded by now
        detector = dlib.get_frontal_face_detector()
        predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")
        
        if not await self.simulate_calibration(cap):
            cap.release()
            return
        
        mood_response = random.choice(self.MOOD_RESPONSES[self.current_mood])
        self.update_callback("Serina", mood_response)
        await self.speak(mood_response)
        
        gesture = self.detect_head_gesture(cap, detector, predictor)
        
        if gesture == "yes":
            if self.current_mood == "angry":
                joke = self.get_ai_joke()
                response = f"Here's something to lighten your mood, my fierce one: {joke}"
                self.update_callback("Serina", response)
                await self.speak(response)
            else:
                response = "I'll play a beautiful song for you, my love."
                self.update_callback("Serina", response)
                await self.speak(response)
                
                if self.current_mood == "sad":
                    song_url = random.choice(self.SAD_SONG_URLS)
                    self.open_youtube_url(song_url)
                elif self.current_mood == "bored":
                    song_url = random.choice(self.HAPPY_SONG_URLS)
                    self.open_youtube_url(song_url)
                else:
                    song_url = random.choice(self.HAPPY_SONG_URLS)
                    self.open_youtube_url(song_url)
        elif gesture == "no":
            response = "Alright, let's just chat then, my precious."
            self.update_callback("Serina", response)
            await self.speak(response)
        else:
            response = "I'm not sure what you want, darling. Let's just talk."
            self.update_callback("Serina", response)
            await self.speak(response)

        self.conversation.append({"role": "user", "content": "You asked if I wanted music or a joke."})
        self.conversation.append({"role": "assistant", "content": mood_response})
        self.conversation.append({"role": "user", "content": f"I responded with {gesture if gesture else 'no clear response'}"})
        self.conversation.append({"role": "assistant", "content": response})

        async def process_frame():
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                    
                rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                hand_results = self.hands.process(rgb_image)
                
                if hand_results.multi_hand_landmarks:
                    for hand_landmarks in hand_results.multi_hand_landmarks:
                        landmarks = hand_landmarks.landmark
                        index_extended = landmarks[8].y < landmarks[6].y and landmarks[6].y < landmarks[5].y
                        middle_folded = landmarks[12].y > landmarks[10].y
                        ring_folded = landmarks[16].y > landmarks[14].y
                        pinky_folded = landmarks[20].y > landmarks[18].y
                        
                        if index_extended and middle_folded and ring_folded and pinky_folded:
                            self.update_callback("Action", "1 finger detected - switching to face control mode")
                            cap.release()
                            return True
                
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                faces = detector(gray)
                
                for face in faces:
                    landmarks = predictor(gray, face)
                
                await asyncio.sleep(0.01)
            return False
        
        frame_task = asyncio.create_task(process_frame())
        
        while True:
            if frame_task.done() and frame_task.result():
                break
            
            user_input = self.listen()
            if not user_input:
                continue

            youtube_handled = await self.handle_youtube_request(user_input)
            if youtube_handled:
                continue
            
            self.conversation.append({"role": "user", "content": user_input})
            reply = self.get_ai_reply(self.conversation)

            self.update_callback("Serina", reply)
            await self.speak(reply)
            self.conversation.append({"role": "assistant", "content": reply})

        frame_task.cancel()
        try:
            await frame_task
        except asyncio.CancelledError:
            pass

    # ======================== Phase 2 Methods ========================
    def calculate_ear(self, eye_landmarks, frame_width, frame_height):
        """Calculate Eye Aspect Ratio"""
        points = []
        for landmark in eye_landmarks:
            x = int(landmark.x * frame_width)
            y = int(landmark.y * frame_height)
            points.append([x, y])
        
        vertical_dist1 = np.linalg.norm(np.array(points[1]) - np.array(points[5]))
        vertical_dist2 = np.linalg.norm(np.array(points[2]) - np.array(points[4]))
        horizontal_dist = np.linalg.norm(np.array(points[0]) - np.array(points[3]))
        
        if horizontal_dist == 0:
            return 0
        ear = (vertical_dist1 + vertical_dist2) / (2.0 * horizontal_dist)
        return ear

    def detect_eyebrow_raise(self, face_landmarks, threshold=0.03):
        """Detect eyebrow raise"""
        try:
            right_eyebrow = face_landmarks[282]
            right_eye_top = face_landmarks[159]
            left_eyebrow = face_landmarks[52]
            left_eye_top = face_landmarks[386]
            
            right_distance = right_eyebrow.y - right_eye_top.y
            left_distance = left_eyebrow.y - left_eye_top.y
            
            return (right_distance < -threshold) and (left_distance < -threshold)
        except IndexError:
            return False

    def open_application(self, app):
        """Open a web application"""
        try:
            if app == "whatsapp" and self.whatsapp_count < self.max_tabs_per_app:
                webbrowser.open("https://web.whatsapp.com")
                self.whatsapp_count += 1
                self.update_callback("Action", f"Opening WhatsApp (Tab {self.whatsapp_count}/{self.max_tabs_per_app})")
            elif app == "youtube" and self.youtube_count < self.max_tabs_per_app:
                webbrowser.open("https://www.youtube.com")
                self.youtube_count += 1
                self.update_callback("Action", f"Opening YouTube (Tab {self.youtube_count}/{self.max_tabs_per_app})")
            elif app == "gmail" and self.gmail_count < self.max_tabs_per_app:
                webbrowser.open("https://mail.google.com")
                self.gmail_count += 1
                self.update_callback("Action", f"Opening Gmail (Tab {self.gmail_count}/{self.max_tabs_per_app})")
            elif app == "instagram" and self.instagram_count < self.max_tabs_per_app:
                webbrowser.open("https://www.instagram.com")
                self.instagram_count += 1
                self.update_callback("Action", f"Opening Instagram (Tab {self.instagram_count}/{self.max_tabs_per_app})")
            elif app in ["whatsapp", "youtube", "gmail", "instagram"]:
                self.update_callback("Warning", f"Maximum limit of {self.max_tabs_per_app} tabs reached for {app}")
            
        except Exception as e:
            self.update_callback("Error", f"Failed to open {app}: {e}")

    def count_fingers(self, hand_landmarks):
        """Count extended fingers"""
        if not hand_landmarks:
            return 0
            
        try:
            landmarks = hand_landmarks.landmark
            extended_fingers = 0
            
            if landmarks[4].x < landmarks[3].x:
                extended_fingers += 1
            
            finger_tips = [8, 12, 16, 20]
            mcp_joints = [5, 9, 13, 17]
            
            for i, tip_idx in enumerate(finger_tips):
                pip_idx = tip_idx - 2
                mcp_idx = mcp_joints[i]
                
                if landmarks[tip_idx].y < landmarks[pip_idx].y and landmarks[pip_idx].y < landmarks[mcp_idx].y:
                    extended_fingers += 1
                
            return extended_fingers
        except (IndexError, AttributeError):
            return 0

    def is_peace_sign(self, hand_landmarks):
        """Detect peace sign gesture"""
        if not hand_landmarks:
            return False
            
        try:
            landmarks = hand_landmarks.landmark
            index_extended = landmarks[8].y < landmarks[6].y and landmarks[6].y < landmarks[5].y
            middle_extended = landmarks[12].y < landmarks[10].y and landmarks[10].y < landmarks[9].y
            ring_folded = landmarks[16].y > landmarks[14].y
            pinky_folded = landmarks[20].y > landmarks[18].y
            
            return index_extended and middle_extended and ring_folded and pinky_folded
        except (IndexError, AttributeError):
            return False

    def is_index_finger_raised(self, hand_landmarks):
        """Check if only index finger is raised"""
        if not hand_landmarks:
            return False
            
        try:
            landmarks = hand_landmarks.landmark
            index_extended = landmarks[8].y < landmarks[6].y and landmarks[6].y < landmarks[5].y
            middle_folded = landmarks[12].y > landmarks[10].y
            ring_folded = landmarks[16].y > landmarks[14].y
            pinky_folded = landmarks[20].y > landmarks[18].y
            
            return index_extended and middle_folded and ring_folded and pinky_folded
        except (IndexError, AttributeError):
            return False

    def is_three_fingers_raised(self, hand_landmarks):
        """Check if three fingers are raised"""
        if not hand_landmarks:
            return False
            
        try:
            landmarks = hand_landmarks.landmark
            thumb_folded = landmarks[4].x > landmarks[3].x
            index_extended = landmarks[8].y < landmarks[6].y
            middle_extended = landmarks[12].y < landmarks[10].y
            ring_extended = landmarks[16].y < landmarks[14].y
            pinky_folded = landmarks[20].y > landmarks[18].y
            
            return thumb_folded and index_extended and middle_extended and ring_extended and pinky_folded
        except (IndexError, AttributeError):
            return False

    def eye_aspect_ratio(self, landmarks, eye_indices):
        """Calculate eye aspect ratio for blink detection"""
        v1 = np.array([landmarks[eye_indices[1]].x, landmarks[eye_indices[1]].y])
        v2 = np.array([landmarks[eye_indices[5]].x, landmarks[eye_indices[5]].y])
        
        h1 = np.array([landmarks[eye_indices[0]].x, landmarks[eye_indices[0]].y])
        h2 = np.array([landmarks[eye_indices[3]].x, landmarks[eye_indices[3]].y])
        
        vertical_dist = np.linalg.norm(v1 - v2)
        horizontal_dist = np.linalg.norm(h1 - h2)
        
        ear = vertical_dist / horizontal_dist
        return ear

    def serina_face_mode_main(self):
        """Phase 2: Advanced face control system"""
        # State variables
        last_left_eye_blink_time = time.time()
        last_right_eye_blink_time = time.time()
        last_both_eyes_blink_time = time.time()
        last_eyebrow_raise_time = time.time()
        last_gesture_time = time.time()
        blink_cooldown = 1.0
        eyebrow_cooldown = 2.0
        gesture_cooldown = 1.5
        
        # Eye control variables
        LEFT_IRIS_INDICES = [474, 475, 476, 477]
        RIGHT_IRIS_INDICES = [469, 470, 471, 472]
        last_blink_time = time.time()
        blink_count = 0
        blink_reset_time = 1.0
        click_threshold = 2
        smoothing_factor = 0.5
        prev_x, prev_y = 0, 0
        
        # Calibration variables
        calibration_stage = 0
        is_calibrated = False
        calibration_points = []
        calibration_positions = [(0.1, 0.1), (0.9, 0.1), (0.1, 0.9), (0.9, 0.9), (0.5, 0.5)]
        eye_pos_min_x, eye_pos_max_x = float('inf'), float('-inf')
        eye_pos_min_y, eye_pos_max_y = float('inf'), float('-inf')
        
        # Initialize webcam
        cap = None
        start_time = time.time()
        timeout = 5
        
        while time.time() - start_time < timeout:
            cap = cv2.VideoCapture(0)
            if cap.isOpened():
                break
            time.sleep(0.1)
        
        if not cap or not cap.isOpened():
            self.update_callback("Error", "Could not open camera - please check connection")
            return
            
        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        if fps <= 0:
            fps = 30
        
        screen_width, screen_height = pyautogui.size()
        blink_threshold = 0.25
        
        # Calibration
        calibration = True
        calibration_start = time.time()
        calibration_period = 5
        left_ear_values = []
        right_ear_values = []
        
        smoothing = 4
        prev_finger_pos = []
        
        left_eye_closed = False
        right_eye_closed = False
        both_eyes_closed = False
        
        blink_confirmation_frames = 1
        left_blink_counter = 0
        right_blink_counter = 0
        both_blink_counter = 0
        
        eyebrow_raise_counter = 0
        eyebrow_confirmation_frames = 5
        previous_eyebrow_raised = False
        
        two_finger_counter = 0
        two_finger_confirmation_frames = 3
        
        three_finger_counter = 0
        three_finger_confirmation_frames = 3
        
        self.update_callback("Starting", "Face control system initializing...")
        
        try:
            while cap.isOpened() and self.running:
                success, image = cap.read()
                if not success:
                    self.update_callback("Error", "Failed to get frame from camera")
                    continue
                    
                image = cv2.flip(image, 1)
                rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                
                face_results = self.face_mesh.process(rgb_image)
                hand_results = self.hands.process(rgb_image)
                
                current_time = time.time()
                
                if calibration:
                    if current_time - calibration_start > calibration_period:
                        calibration = False
                        if left_ear_values and right_ear_values:
                            avg_left = sum(left_ear_values) / len(left_ear_values)
                            avg_right = sum(right_ear_values) / len(right_ear_values)
                            blink_threshold = min(avg_left, avg_right) * 0.80
                            self.update_callback("Calibrated", "Calibration complete!")
                        
                        self.update_callback("Instruction", "Commands:")
                        self.update_callback("Instruction", "- Raise Eyebrows: Open WhatsApp")
                        self.update_callback("Instruction", "- Left Eye Blink: Open YouTube")
                        self.update_callback("Instruction", "- Right Eye Blink: Open Gmail")
                        self.update_callback("Instruction", "- Both Eyes Blink: Open Instagram")
                        self.update_callback("Instruction", "- Show 2 Fingers: Toggle control modes")
                        self.update_callback("Instruction", "- Show 3 Fingers: Toggle eye control")
                        self.update_callback("Instruction", "- Single Finger: Control mouse cursor")
                        self.update_callback("Instruction", "- Show 5 Fingers: Exit program")
                    
                    elif face_results.multi_face_landmarks:
                        for face_landmarks in face_results.multi_face_landmarks:
                            try:
                                landmarks = face_landmarks.landmark
                                left_eye_landmarks = [landmarks[p] for p in [33, 160, 158, 133, 153, 144]]
                                right_eye_landmarks = [landmarks[p] for p in [362, 385, 387, 263, 373, 380]]
                                
                                left_ear = self.calculate_ear(left_eye_landmarks, frame_width, frame_height)
                                right_ear = self.calculate_ear(right_eye_landmarks, frame_width, frame_height)
                                
                                left_ear_values.append(left_ear)
                                right_ear_values.append(right_ear)
                            except (IndexError, AttributeError):
                                pass
                    
                    progress = min(100, (current_time - calibration_start) / calibration_period * 100)
                    cv2.putText(image, f"Calibrating: {progress:.0f}%", (10, 30), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                
                else:
                    current_finger_control = False
                    peace_sign_detected = False
                    three_fingers_detected = False
                    
                    if hand_results.multi_hand_landmarks:
                        for hand_landmarks in hand_results.multi_hand_landmarks:
                            self.mp_drawing.draw_landmarks(
                                image,
                                hand_landmarks,
                                self.mp_hands.HAND_CONNECTIONS,
                                self.mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1),
                                self.mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=1, circle_radius=1)
                            )
                            
                            if self.is_peace_sign(hand_landmarks):
                                peace_sign_detected = True
                            
                            if self.is_three_fingers_raised(hand_landmarks):
                                three_fingers_detected = True
                            
                            finger_count = self.count_fingers(hand_landmarks)
                            if finger_count == 5 and (current_time - last_gesture_time > gesture_cooldown):
                                landmarks = hand_landmarks.landmark
                                thumb_extended = landmarks[4].x < landmarks[3].x
                                index_extended = landmarks[8].y < landmarks[6].y
                                middle_extended = landmarks[12].y < landmarks[10].y
                                ring_extended = landmarks[16].y < landmarks[14].y
                                pinky_extended = landmarks[20].y < landmarks[18].y
                                
                                if thumb_extended and index_extended and middle_extended and ring_extended and pinky_extended:
                                    self.update_callback("Exiting", "Face control system shutting down")
                                    cap.release()
                                    cv2.destroyAllWindows()
                                    return
                            
                            if self.finger_control_active and self.is_index_finger_raised(hand_landmarks):
                                current_finger_control = True
                                index_tip = hand_landmarks.landmark[8]
                                x = int(index_tip.x * frame_width)
                                y = int(index_tip.y * frame_height)
                                cv2.circle(image, (x, y), 8, (255, 0, 0), -1)
                                prev_finger_pos.append((x, y))
                                if len(prev_finger_pos) > smoothing:
                                    prev_finger_pos.pop(0)
                                
                                if prev_finger_pos:
                                    avg_x = sum([x for x, y in prev_finger_pos]) / len(prev_finger_pos)
                                    avg_y = sum([y for x, y in prev_finger_pos]) / len(prev_finger_pos)
                                    screen_x = np.interp(avg_x, [50, frame_width - 50], [0, screen_width])
                                    screen_y = np.interp(avg_y, [50, frame_height - 50], [0, screen_height])
                                    pyautogui.moveTo(screen_x, screen_y, duration=0.0)
                    
                    # Handle peace sign gesture (toggle between app and finger control)
                    if peace_sign_detected:
                        two_finger_counter += 1
                    else:
                        two_finger_counter = 0
                    
                    if two_finger_counter >= two_finger_confirmation_frames and (current_time - last_gesture_time > gesture_cooldown):
                        if self.finger_control_active:
                            self.finger_control_active = False
                            self.app_control_active = True
                            self.eye_control_active = False
                            self.update_callback("Mode", "App control activated")
                        else:
                            self.finger_control_active = True
                            self.app_control_active = False
                            self.eye_control_active = False
                            self.update_callback("Mode", "Finger control activated")
                        last_gesture_time = current_time
                        two_finger_counter = 0
                    
                    # Handle three fingers gesture (toggle eye control)
                    if three_fingers_detected:
                        three_finger_counter += 1
                    else:
                        three_finger_counter = 0
                    
                    if three_finger_counter >= three_finger_confirmation_frames and (current_time - last_gesture_time > gesture_cooldown):
                        if self.eye_control_active:
                            self.eye_control_active = False
                            self.app_control_active = True
                            self.finger_control_active = False
                            self.update_callback("Mode", "Eye control deactivated, App control activated")
                        else:
                            self.eye_control_active = True
                            self.app_control_active = False
                            self.finger_control_active = False
                            self.update_callback("Mode", "Eye control activated")
                        last_gesture_time = current_time
                        three_finger_counter = 0
                    
                    if face_results.multi_face_landmarks and self.face_control_active:
                        for face_landmarks in face_results.multi_face_landmarks:
                            self.mp_drawing.draw_landmarks(
                                image,
                                face_landmarks,
                                self.mp_face_mesh.FACEMESH_CONTOURS,
                                self.mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1),
                                self.mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=1, circle_radius=1)
                            )
                            
                            if self.eye_control_active:
                                landmarks = face_landmarks.landmark
                                h, w = image.shape[:2]
                                
                                # Get iris landmarks for gaze tracking
                                left_iris = [(landmarks[idx].x * w, landmarks[idx].y * h) for idx in LEFT_IRIS_INDICES]
                                right_iris = [(landmarks[idx].x * w, landmarks[idx].y * h) for idx in RIGHT_IRIS_INDICES]
                                
                                # Calculate center of each iris
                                left_iris_center = np.mean(left_iris, axis=0)
                                right_iris_center = np.mean(right_iris, axis=0)
                                
                                # Calculate average iris position
                                iris_x = (left_iris_center[0] + right_iris_center[0]) / 2
                                iris_y = (left_iris_center[1] + right_iris_center[1]) / 2
                                
                                # Normalize coordinates
                                norm_x = iris_x / w
                                norm_y = iris_y / h
                                
                                # Map to screen coordinates
                                if is_calibrated:
                                    x_range = eye_pos_max_x - eye_pos_min_x
                                    y_range = eye_pos_max_y - eye_pos_min_y
                                    if x_range > 0 and y_range > 0:
                                        cursor_x = (norm_x - eye_pos_min_x) / x_range * screen_width
                                        cursor_y = (norm_y - eye_pos_min_y) / y_range * screen_height
                                else:
                                    # Fallback without calibration
                                    cursor_x = norm_x * screen_width
                                    cursor_y = norm_y * screen_height
                                
                                # Apply smoothing
                                cursor_x = prev_x * smoothing_factor + cursor_x * (1 - smoothing_factor)
                                cursor_y = prev_y * smoothing_factor + cursor_y * (1 - smoothing_factor)
                                
                                # Update cursor position
                                pyautogui.moveTo(int(cursor_x), int(cursor_y))
                                prev_x, prev_y = cursor_x, cursor_y
                                
                                # Blink detection
                                left_ear = self.eye_aspect_ratio(landmarks, [33, 160, 158, 133, 153, 144])
                                right_ear = self.eye_aspect_ratio(landmarks, [362, 385, 387, 263, 373, 380])
                                avg_ear = (left_ear + right_ear) / 2
                                
                                if avg_ear < blink_threshold:
                                    current_time = time.time()
                                    if current_time - last_blink_time < blink_reset_time:
                                        blink_count += 1
                                    else:
                                        blink_count = 1
                                    last_blink_time = current_time
                                    
                                    if blink_count >= click_threshold:
                                        pyautogui.click()
                                        blink_count = 0
                                        time.sleep(0.5)
                                
                                # Display blink count
                                if blink_count > 0 and (current_time - last_blink_time < blink_reset_time):
                                    cv2.putText(image, f"Blinks: {blink_count}", (frame_width - 200, 60), 
                                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
                            
                            try:
                                landmarks = face_landmarks.landmark
                                left_eye_landmarks = [landmarks[p] for p in [33, 160, 158, 133, 153, 144]]
                                right_eye_landmarks = [landmarks[p] for p in [362, 385, 387, 263, 373, 380]]
                                
                                left_ear = self.calculate_ear(left_eye_landmarks, frame_width, frame_height)
                                right_ear = self.calculate_ear(right_eye_landmarks, frame_width, frame_height)
                                
                                if self.app_control_active:
                                    eyebrow_raised = self.detect_eyebrow_raise(landmarks)
                                    
                                    if eyebrow_raised:
                                        eyebrow_raise_counter += 1
                                    else:
                                        eyebrow_raise_counter = 0
                                    
                                    if eyebrow_raise_counter > 0:
                                        cv2.putText(image, f"Eyebrow raising: {eyebrow_raise_counter}/{eyebrow_confirmation_frames}", 
                                                   (frame_width - 300, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
                                    
                                    if eyebrow_raise_counter >= eyebrow_confirmation_frames:
                                        if not previous_eyebrow_raised and (current_time - last_eyebrow_raise_time > eyebrow_cooldown):
                                            if left_ear > blink_threshold * 1.2 and right_ear > blink_threshold * 1.2:
                                                self.open_application("whatsapp")
                                                last_eyebrow_raise_time = current_time
                                                previous_eyebrow_raised = True
                                    else:
                                        previous_eyebrow_raised = False
                                    
                                    if left_ear < blink_threshold and right_ear > blink_threshold:
                                        left_blink_counter += 1
                                        if left_blink_counter >= blink_confirmation_frames:
                                            if not left_eye_closed and (current_time - last_left_eye_blink_time > blink_cooldown):
                                                self.open_application("youtube")
                                                last_left_eye_blink_time = current_time
                                            left_eye_closed = True
                                    else:
                                        left_eye_closed = False
                                        left_blink_counter = 0
                                    
                                    if right_ear < blink_threshold and left_ear > blink_threshold:
                                        right_blink_counter += 1
                                        if right_blink_counter >= blink_confirmation_frames:
                                            if not right_eye_closed and (current_time - last_right_eye_blink_time > blink_cooldown):
                                                self.open_application("gmail")
                                                last_right_eye_blink_time = current_time
                                            right_eye_closed = True
                                    else:
                                        right_eye_closed = False
                                        right_blink_counter = 0
                                    
                                    if left_ear < blink_threshold and right_ear < blink_threshold:
                                        both_blink_counter += 1
                                        if both_blink_counter >= blink_confirmation_frames:
                                            if not both_eyes_closed and (current_time - last_both_eyes_blink_time > blink_cooldown):
                                                self.open_application("instagram")
                                                last_both_eyes_blink_time = current_time
                                                both_eyes_closed = True
                                    else:
                                        both_eyes_closed = False
                                        both_blink_counter = 0
                                
                                if current_finger_control:
                                    if (left_ear < blink_threshold*1.1 or right_ear < blink_threshold*1.1):
                                        if left_ear < blink_threshold*1.1:
                                            left_blink_counter += 1
                                            if left_blink_counter >= blink_confirmation_frames:
                                                if current_time - last_left_eye_blink_time > blink_cooldown:
                                                    pyautogui.click()
                                                    last_left_eye_blink_time = current_time
                                        else:
                                            right_blink_counter += 1
                                            if right_blink_counter >= blink_confirmation_frames:
                                                if current_time - last_right_eye_blink_time > blink_cooldown:
                                                    pyautogui.click()
                                                    last_right_eye_blink_time = current_time
                                    else:
                                        left_blink_counter = 0
                                        right_blink_counter = 0
                                            
                                cv2.putText(image, f"Left EAR: {left_ear:.2f}", (10, 120), 
                                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 1)
                                cv2.putText(image, f"Right EAR: {right_ear:.2f}", (10, 150), 
                                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 1)
                                cv2.putText(image, f"Blink threshold: {blink_threshold:.2f}", (10, 180), 
                                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 1)
                                         
                            except (IndexError, AttributeError) as e:
                                pass
                    
                    app_status = "App Control: ACTIVE" if self.app_control_active else "App Control: INACTIVE"
                    finger_status = "Finger Control: ACTIVE" if self.finger_control_active else "Finger Control: INACTIVE"
                    eye_status = "Eye Control: ACTIVE" if self.eye_control_active else "Eye Control: INACTIVE"
                    
                    cv2.putText(image, app_status, (10, 30), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0) if self.app_control_active else (0, 0, 255), 2)
                    cv2.putText(image, finger_status, (10, 60), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0) if self.finger_control_active else (0, 0, 255), 2)
                    cv2.putText(image, eye_status, (10, 90), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0) if self.eye_control_active else (0, 0, 255), 2)
                    
                    if self.eye_control_active:
                        cv2.putText(image, "Double blink to click", (10, 240), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 1)
                    
                    cv2.putText(image, f"Apps: W:{self.whatsapp_count}/{self.max_tabs_per_app} Y:{self.youtube_count}/{self.max_tabs_per_app} G:{self.gmail_count}/{self.max_tabs_per_app} I:{self.instagram_count}/{self.max_tabs_per_app}", 
                                (10, 270), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 1)
                
                cv2.imshow('Enhanced Face Control System', image)
                
                key = cv2.waitKey(5) & 0xFF
                if key == ord('c') and not calibration_stage:
                    calibration_stage = 1
                    calibration_points = []
                    eye_pos_min_x, eye_pos_max_x = float('inf'), float('-inf')
                    eye_pos_min_y, eye_pos_max_y = float('inf'), float('-inf')
                    is_calibrated = False
                elif key == 27 or key == ord('q'):
                    break
                
        except Exception as e:
            self.update_callback("Error", f"Error in face control: {str(e)}")
        finally:
            cap.release()
            cv2.destroyAllWindows()
            self.running = False